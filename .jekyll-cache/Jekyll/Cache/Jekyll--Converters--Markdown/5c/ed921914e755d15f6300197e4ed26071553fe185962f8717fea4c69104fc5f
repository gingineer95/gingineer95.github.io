I"◊<div align="center"><em>GIF above is playing at 4x speed</em></div>
<p>&nbsp;</p>

<h1 id="project-overview">Project Overview:</h1>
<p>In this project, my team programmed a Rethink Baxter robot to segment and pick bottles and cans then drop them into separate recycling bins. We used OpenCV to detect and locate the randomly placed bottles and cans, while for the robotic manipulation we utilized MoveIt!. Take a look at the code on my <a href="https://github.com/gingineer95/baxter-recycling-segmentation" target="_blank" rel="noopener noreferrer">GitHub</a>.</p>

<p>Below is a video (4x speed) of the project. The video starts by showing the bottles and cans being placed, then a quick view of the computer vision, and lastly it displays two different views of the robot in action.</p>

<iframe width="750" height="400" src="https://youtu.be/0IDe7L2YoR4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
<p>&nbsp;</p>

<h2 id="moveit">MoveIt!</h2>
<p>The robot operation was controlled entirely through ROS‚Äôs MoveIt! library for motion planning and manipulation (mainly the compute_cartesian_path command). After initalizing the Move Group Commander for Baxter‚Äôs right arm, a table is added to the Planning Scence to ensure that the robot does not collide with the table.</p>

<p>The arm then moves to a position out of the camera‚Äôs field of view and calls the camera‚Äôs segmentation service. Once the objects have been located and classfied, the arm moves to a predetermined home position over the objects to ensure smooth and predictable motion of the arm (This home configuration was determined after testing).</p>

<h2 id="computer-vision">Computer Vision</h2>
<p>A Realsense camera was used to take real-time video of the objects placed in front of the robot. After calibrating the camera, we used OpenCV and specifically Hough Circles to identify, locate and classify the bottles and cans. The camera segmentation service returns a list with the position and classification of all bottles and cans in the camera‚Äôs view.</p>

<p><img src="/gingineer95.github.io/kaileysmith/images/computer_vision.png" alt="" /></p>

<h2 id="action-sequence">Action Sequence</h2>
<ol>
  <li>Baxter‚Äôs right arm moves to the home position where the robot is safely above all objects.</li>
  <li>A livestream of the camera opens that captures, segments and locates all objects on the table. (For this project, we only have bottles and cans)</li>
  <li>For an object on the table, the robot‚Äôs head will display either the can image or the bottle image, depending on the classification.</li>
  <li>Next, Baxter‚Äôs right arm will move to object‚Äôs (x,y) corrdinate at a safe z height away. This is the same height for bottles and cans.</li>
  <li>The arm then moves down to the appropriate perch height, depending on classification. (For example, the robot arm will be position further away from the table for bottle, since those are taller than cans).</li>
  <li>Once safely at the perch height, the ame moves down again. This time aligning the object in between the grippers.</li>
  <li>Baxter‚Äôs arm then grasps the object.</li>
  <li>The arm moves back up to the ‚Äúsafe position‚Äù; the same position as step 4.</li>
  <li>Baxter now moves back to the home position. This step was added to ensure predictable behavior of the robot arm.</li>
  <li>Depending on the object‚Äôs classification, the arm will move to the appropriate bin. Baxter‚Äôs head will also display the recylcing image.</li>
  <li>Once over the bin, Baxter opens its grippers and drops the object. To show that the object has been recycled, Baxter displays the bin image.</li>
  <li>Steps 3 through 11 are repeated for all objects found.</li>
</ol>
:ET